Benedict NeoFollowbitgrit Data Science Publication--ListenShareRegression modeling is a powerful tool that can be applied to many events in real life. It's heavily used in many companies on all sorts of business issues.Regression can be used to explain a certain event, e.g., why sales dropped last month; make predictions, e.g., what sales will be in the next five months; and to make decisions, e.g., should we implement this or that marketing strategy.The fundamental idea of regression is to understand and estimate the relationship between independent variables (promotions, holidays, advertisements, etc.) and dependent variables (monthly sales).Doing regression analysis can help us determine what variables impact the monthly sales, answering questions like "Which factors are do we keep and throw away," "how do these factors interact with each other?" etc.Many statistics textbook you read on regression focuses on math and provide simple examples that aren't realistic. Real-world statistics, however, is complex.I've been looking for a book that can provide me with insights into how regression is used to do estimations and predictions in the real world.The search ended when I stumbled upon the book Regression and Other Stories by Professor Andrew Gelman, Jennifer Hill, Aki Vehtari.Andrew Gelman also writes on the popular blog "Statistical Modeling, Causal Inference, and Social Science," which talks about everything from Causal Inference and Bayesian statistics to Political science and Sports.The book is awesome as it focuses on practical issues such as missing data and provides a wide range of techniques to solve them.The appendix section provides ten quick tips to improve your regression modeling, which I found very insightful.So, in this article, I will be sharing those tips and include some of my takeaways.Be sure to check out the book's website to further improve your regression game!Subscribe to our AI Newsletter  Deep Grit.Want to read it first? Check out our latest issue!Before we get into the tips, let's make sure we understand the assumptions of the regression model.In decreasing order of importance :We are taught in statistics class about the importance of the variation in the error term, also known as residual variance.However, variance is also important for the model in general.Variation is central to regression modeling, and not just in the error termIt's beneficial to fit the model to various datasets that the model has not seen before, as this produces variance in the relationship between the variables.This is more useful than only having the standard error from one study, as it gives a sense of variation across different problems.Another important aspect is replication, which the book notes as an ideal replication, "performing all the steps of a study from scratch, not just increasing sample size and collecting more data within an existing setting."Doing replication from scratch allows you to capture variation based on different aspects of data collection and measurement, breaking away from the bubble of results seen in a single study.p-values are painted to be the golden rule of determining whether a test has significant results or not, but p  0.05 doesn't tell the full story.Forget about p-values and whether your confidence intervals exclude zero  Regression and Other StoriesThe book states three key reasons to forget about statistical significance:Why?  "Measures of significance such as p-values are noisy, and it is misleading to treat an experiment as a success or failure based on a significance test."Why?  "No true populations are identical, and anything that plausibly could have an effect will not have an effect that is exactly zero."Why?  Having a confidence interval that excludes zero doesn't tell us something useful about future differences.Graphing is one of the most crucial steps in any analysis.The goal of any graph is communication to self or othersThe first step is to always display raw data (EDA), where the goal is to see things you did not expect or even know to look for.But this begs the question: What is relevant to visualize and not?A table of regression coefficients does not tell the same story as the visualizations and graphs.You can graph the fitted models a couple of ways:Find examples of these in the book in C10 and C11.Real-world data is high in dimension and complex.It's crucial to make many different graphs of the data and look at the model from different angles.Instead of single images, use a series of graphs and tell a story through the visualizations.What is irrelevant?Many statistics textbooks and classes focus strongly on plots for raw data and regression diagnostics (QQ plot, residual variance)Although they are relevant when evaluating the use of the model for predicting individual data points, they're not useful for satisfying the assumptions of representativeness, validity, additivity, linearity, etc., in regression.Rule: Be prepared to explain any graph you showThis tip changed how I view regression coefficients.Take this linear regression equation as an examplesalary = -20 + 0.6 * height + 10.6 * male + error where salary is per $1k and height is in inches from page 84 of Regression and Other StoriesIt might seem natural to report that the estimated effect of height is 0.6 or 600$ in this case.However, it's not appropriate to see them as "effects", as that would suggest that if we increase someone's height by one inch, their earnings will increase by en estimated amount of $600.What our model is really telling us is that taller people in our sample have higher earnings on average.The correct interpretation is  "under the fitted model, the average difference in earnings, comparing two people of the same sex but one inch different in height is $600."From the data alone, a regression only tells us about comparisons between individuals, not about changes within individualsBenefits of thinking of regression as comparisons:Comparisons can be said as the description of the model and do not require any causal assumptions.We can consider more complicated regressions as built up from simpler models, starting with simple comparisons and adding adjustments.The comparative interpretation also works in the special case of causal inference, where we can consider comparisons between the same individual receiving two different levels of a treatment.Fake-data simulation  Simulate fake data given an assumed generative model to evaluate the properties of statistical methods and procedures being usedGenerative models  A model that includes the distribution of the data itself, and tells you how likely a given example is.Why use fake-data simulation? 4 reasons.The decisions made in constructing a simulated world based on the model can clarify the following:Put the simulation and inference into a loop, and you can see how close the model's estimates and predictions are to the assumed true values.Here it can make sense to simulate from a process that includes features not included in the model you will use to fit the data  but, again, this can be a good thing in that it forces you to consider assumptions that might be violated.With large samples or small data variance, your fitted model should be able to recover the true parameters.If it can't, you may have a coding problem or a conceptual problem, where your model is not doing what you think it is doing.It can help in such settings to plot the simulated data overlaid with the assumed and fitted models.You need fake-data simulation if you want to design a new study and collect new data with some reasonable expectation of what you might find.It's generally good to start with a simple model to understand how well the data fits with the models.You can also start with a complex model, drop things out and slowly move to a simpler model.Realistically, you dont know what model you want, so its good to fit models quickly.Another great tip is to keep track of the models you have fit. This helps you understand your data and protect yourself from biases that can arise when you have many ways of analyzing data.Thus, make it a habit to record all the procedures you've done and report the results from all relevant models.Faster and more reliable computation = better statistical workflowif you can fit models faster, you can fit more models and better understand both data and model.There are two approaches:A trick to speed up computations is to break a large dataset into subsets and analyze each separately.Advantage  Faster computation while allowing you to explore the data by trying out more models. Separate analysis that is well executed can also reveal variation across subsets.Disadvantage  (1) Inconvenient to partition data, perform analysis, and summarize the results. (2) Separate analysis may not be as accurate as putting all the data together in a single analysisSolution: Use multilevel modeling to subset without losing inferential efficiency.Fake data and predictive simulation effectively diagnose problems in the code or the model fit.Predictive simulation  Using Bayesian generalized linear models to obtain simulations to make probabilistic predictions(1) Use fake-data simulation to check the correctness of the code(2) Use predictive simulation to compare the data to the fitted model's prediction.Two transformations that are commonly used are:You always hear about log transforming to help deal with skewness.However, it's more important for satisfying the assumptions of validity, additivity, and linearity.For example, if additivity is violated, i.e., instead of y = B0 + B1x1 you have, a log transformation would give you log y = log a + log b + log cIt's also common for linearity to be violated, i.e., nonlinear functions in medical fields where a health measure declines with higher ages, they can be solved by getting the reciprocal of the predictor 1/x or log(x) or with nonlinear functions like splines or Gaussian processStandardizing is useful to keep all the data within a certain scale or range so that statistics can be directly interpreted and scaled.The above transformations were univariate. In addition to that, you can create interactions and engineer new features by combining inputs.This tip warns against assuming that a comparison or regression coefficient can be interpreted causally.The book states that if you are interested in causal inference, consider your treatment variable carefully and use the tools in the book (Chapters 1821) to address the challenges of balance and overlap when comparing treated and control units to estimate a causal effect and its variation across the population.It continues to explain that even if you are using a natural experiment or identification strategy, it is important to compare treatment and control groups and adjust for pre-treatment differences.When considering several causal questions, it can be tempting to set up a single large regression to answer them all at once; however, this is not appropriate in observational settings (including experiments in which certain conditions of interest are observational).This is my favorite tip, and what I think is the best tip, as it leaves the reader with an action plan.If you want to learn and apply complicated statistics methods, apply them to problems you care about.First, gather data on the examples using the right data-collection procedures.This requires being aware of the population of interest.Before you dive into the analysis, determine the larger goals of your data collection and analysis, be specific about what you want to achieve and whether you can achieve it with the data you have.Then, develop statistical understanding of the data through simulation and visualizations.That's all for this article, be sure to check out the book for code examples and explanations that go in-depth into each of these tips avehtari.github.ioAlso, feel free to leave your thoughts on these tips in the comments!Be sure to follow the bitgrit Data Science Publication to keep updated!Want to discuss the latest developments in Data Science and AI with other data scientists? Join our discord server!Follow Bitgrit's socials  to stay updated on workshops and upcoming competitions!----bitgrit Data Science Publicationdata scienceBenedict Neoinbitgrit Data Science Publication--4Benedict Neoinbitgrit Data Science Publication--2Benedict Neoinbitgrit Data Science Publication--12Benedict Neoinbitgrit Data Science Publication--4Erkan HatipoluinTowards AI--Conor O'SullivaninTowards Data Science--1Vedat Gl--Maxim GusarovinCodeX--2Rania SaoudinGoPenAI--3Tasmay Pankaj Tibrewal--2HelpStatusWritersBlogCareersPrivacyTermsAboutText to speechTeams