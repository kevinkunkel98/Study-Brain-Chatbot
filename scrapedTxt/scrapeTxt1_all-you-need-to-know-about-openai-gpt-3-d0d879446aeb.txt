dharmam savaniFollow--1ListenShareRecently OpenAI revealed the GPT-3 model which has created a lot of hype because it is largest model trained yet . GPT-3 is similar to GPT-2 which is made on transformer architecture. GPT-3 was trained on 175 billion parameters, 10x more than any previous non-sparse language model. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.And list goes on..Researchers published paper on GPT-3 named Language Models are Few-Shot Learners where they tested GPT-3 on above given tasks on well known datasets against fine-tuned State of the art models. In some tests GPT-3 beated state of the art models on zero shot configurations.Limitation of existing state of the art models is that it needs to be fine tuned with lots of data for performing specific task to achieve high accuracy. But GPT-3 can perform well without providing examples of that task which they call zero-shot.Lets say we want to translate the word Cheese to french using model.Fine tuning :- Providing model with examples of english to french translations of words. In this method gradient updates are done similar to transfer learning.One-shot / Few-shot :- We can provide model with one or few examples of translations.Zero-shot:- We can prompt the model by writing a command in natural language. Model will figure out what to perform automatically.This is what Humans do regularly.Openai has released APIs for everyone to use GPT-3 . They have made github repository for gpt3 which have some data and stats of model . They have specified reasons why GPT3 can be misused. You can also use API by joining waitlist on website.The ability of GPT-3 to generate several paragraphs of synthetic content that people find difficult to distinguish from human-written text in pose a threat for making model open source. So to closely monitor use of GPT-3 they have made API open to public which can be integrated with websites. They did it to further know the limits at which GPT-3 can operate because checking accuracy in datasets cannot reveal how well it performs in real world scenario.Experiments by researchers showed that model was biased on some specific races. Also most used words for religions were related to terrorism, violence, arrogant people. Which shows that model has biases towards race and religions.Examples includeSome users on Reddit called it proto-AGI which is not true because it is just bigger version of GPT-2 . Also it failed to beat state of the art in many datasets.real world scenarios where GPT-3 will be helpfulThis is how excel should be using Openais API .above video proves that GPT-3 will help users to do faster interactions in excel----1curious about ai || sophomore at ADIT || programmerMaximilian VogelinMLearning.ai--98TeeTracker--1Dominik PolzerinTowards Data Science--43Bebika Singhinreadytowork-org--AL Anany--103The PyCoachinArtificial Corner--568HelpStatusWritersBlogCareersPrivacyTermsAboutText to speechTeams